---
title: "Mixed Models Part 2"
author: ""
date: ""
output: 
  html_document:
    theme: flatly
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Overview 

In this second workshop on mixed models we will look at how to build LMMs for factorial design, including ensuring we choose a contrast coding scheme for our experimental factors that allows us to interpret our parameter estimates appropriately. We'll also look at generalised linear mixed models (GLMMs) in the context of modelling binomial data (where our DV is a 0 or a 1), and ordinal mixed models for cases where our DV is measured on an ordinal scale (as might be the case with Likert-scale data).

<center>

&nbsp;&nbsp;

<iframe width="560" height="315" src="https://youtube.com/embed//h3G3WZFSq34" frameborder="0" data-external="1" allowfullscreen></iframe>

&nbsp;&nbsp;

</center>

## Slides

You can download the slides in .odp format by clicking [here](../slides/Mixed_Models_Part_2.odp) and in .pdf format by clicking on the image below. 

&nbsp;&nbsp;

<center>

[![Link to slides](../images/mixed_models_pt2.png){width=75%}](../slides/Mixed_Models_Part_2.pdf)

</center>

&nbsp;&nbsp;

Once you've watched the video above, run the code below on your own machines.

## Mixed Models
### 2 x 2 Factorial Design

In this first case imagine that we have a 2 x 2 repeated measures design. The first factor is Context (Negative vs. Positive) and the second is Sentence Type (Negative vs. Positive). The DV is reading time duration to a Target Sentence (measured in ms.). We have 60 subjects, and 28 items.

Let's first load the libraries that we're going to use.

```{r, message=FALSE}
library(tidyverse)
library(lme4)
library(lmerTest)
library(emmeans)
library(performance)
library(pbkrtest)
```

Now we're going to read in our data and turn our `subject`, `item`, `context`, and `sentence` columns into factors.

```{r, message=FALSE}
factorial_data <- read_csv("https://raw.githubusercontent.com/ajstewartlang/15_mixed_models_pt1/master/data/2x2.csv")

tidied_factorial_data <- factorial_data %>%
  mutate(subject = factor(Subject), item = factor(Item), RT = RT,
            context = factor(Context), sentence = factor(Sentence))
```

When we generate summary statistics, we encounter the issues of NAs where we expected to see mean values. 

```{r}
tidied_factorial_data %>%
  group_by(context, sentence) %>%
  summarise(mean_rt = mean(RT), sd_rt = sd(RT))
```

This indicates we likely have missing data. R never decides what it should do about missing data - you need to make that decision. Let's use the {visdat} package to investigate...

```{r}
library(visdat)
```

```{r}
vis_miss(tidied_factorial_data)
```

We can calculate how much data RT is missing another way... 

```{r}
tidied_factorial_data %>%
  filter(is.na(RT)) %>%
  count()
```

So we can see we have 12 missing data points. We can filter our cases where we have missing data and re-do our summary statistics.

```{r}
tidied_factorial_data %>%
  filter(!is.na(RT)) %>%
  group_by(context, sentence) %>%
  summarise(mean_rt = mean(RT), sd_rt = sd(RT))
```

Let's visualise the data next. Using the `stat_summary()` function lets us add means and bootstrapped condidence intervals around those means for each of our conditions. The syntax `context:sentence` in the `aes()` expression indicates that it should plot all combinations of these two factors (i.e., the interactions). There are a few other aesthetic tweaks used to make the graph look a little better than the default. Another alteration would be to add a title - can you modify the code below to do that?

```{r}
tidied_factorial_data %>%
  filter(!is.na(RT)) %>%
  ggplot(aes(x = context:sentence, y = RT, colour = context:sentence)) +
  geom_violin() +
  geom_jitter(width = .1, alpha = .2) +
  stat_summary(fun.data = "mean_cl_boot", colour = "black") +
  guides(colour = FALSE) +
  labs(x = "Context X Sentence",
       y = "RT (ms.)") +
  theme_minimal() +
  coord_flip()
```

Before we build our model, we need to set the contrast coding for our factors. By default, R using dummy (treatment) coding (think back to how we used this in the context of understanding ANOVA as a case of regression). The problem with dummy coding for factorial designs is that you can end up misinterpreting simple effects (e.g., an effect of Factor 2 at one level of Factor 1) as a main effect (e.g., an effect of Factor 2 at the average of Factor 1). To address this, we can use sum or deviation coding. This will result in the Intercept of our model correspoding to the grand mean of our conditions (i.e., the mean of means) and makes the interpretation of our fixed effects (and any interaction effects) more straightforward. 

If you're interested in reading more about this topic, see this great paper by Schad and colleagues. Just click on the image below to be access it.

<center>

[![Link to Schad paper](../images/schad_et_al.png){width=75%}](https://www.sciencedirect.com/science/article/pii/S0749596X19300695)

</center>

Let's set the contrast coding of our factors using deviation coding as follows. This will allow us to compare each of our conditions to the average of the other conditions.

```{r}
contrasts(tidied_factorial_data$context) <- matrix(c(.5, -.5))
contrasts(tidied_factorial_data$sentence) <- matrix(c(.5, -.5))
```

Now that our contrasts are coded we can go ahead and build our mixed model. Note that the maximal model did not converge so we dropped the interaction term (`context:sentence`) from our subject random effect. For this random effect, we are modelling just additive effects of `context` and `sentence`.

```{r}
factorial_model <- lmer(RT ~ context * sentence + 
                          (1 + context + sentence | subject) +
                          (1 + context * sentence | item), 
                        data = tidied_factorial_data)
```

Let's check the assumptions of our model.

```{r}
check_model(factorial_model)
```

So it looks like we may have an issue with the normality of our residuals. Things looks ok apart from the right 20% of our residuals. We may want to try to model under a different distribution. We can plot our RT values on a Cullen and Frey plot.

```{r, warning=FALSE, message=FALSE}
library(fitdistrplus)
```

```{r}
missing_data_removed <- tidied_factorial_data %>%
  filter(!is.na(RT))
  
descdist(missing_data_removed$RT)
```

On the Cullen and Frey plot we see our data is quite close to a Gamma distribution. We can try to model our data using a generalised linear model assuming sampling from the Gamma distribution as follows. One of the challenges with such models is that very often the random effects structure needs to be radically simplified. This can increase the Type I error rate and may result in us thinking we have an effect when really we don't. This is one of the challenges of building models in general. To paraphrase George Box, all models are wrong but some are useful. Let's try to build a Gamma model anyway. 

```{r}
gamma_factorial_model <- glmer(RT ~ context * sentence + 
                          (1 + context + sentence | subject) +
                          (1 + sentence | item), 
                          family = Gamma,
                          nAGQ = 0,
                          data = tidied_factorial_data)
```

In order to fit this model, we had to simplify the random effects terms and set nAGQ to 0 (its default is 1). This means our parameter estimates are a little less exact than if we had gone with the default (but at least the model converges on a solution). 

Let's look at the summaries of our two models and see if they differ. First we'll look at our `factorial_model` which we built using `lmer()`.

```{r}
summary(factorial_model)
```
So we have an interaction between `Context` and `Sentence`.

What about our Gamma model built using `glmer()`?

```{r}
summary(gamma_factorial_model)
```

OK, this is interesting. The interaction between `Context` and `Sentence` is still there. You are probably fine to report either the results of the LMM or the GLMM. Whichever you choose, be transparent and open when you report them. Indeed, you might even want to report both models but highlight the fact that the interaction term is significant regardless of which model is chosen.

To interpret the interaction, you'll need to run pairwise comparisons using `emmeans()`.

```{r}
emmeans(factorial_model, pairwise ~ context*sentence, adjust = "none")
```

We can see the interaction is being driven by reading times to Negative sentences in Negative vs. Positive contexts. 

Is this also the case for our Gamma model?

```{r}
emmeans(gamma_factorial_model, pairwise ~ context*sentence, adjust = "none")
```

We see here that the pattern of pairwise comparisons is the same as with our previous linear model - the interaction is still being driven by reading times to Negative sentences in Negative vs. Positive contexts.

## Binomial Data

The following video will examine how you build a generalised linear mixed model where your dependent variable isn't continuous. We will focus on binomial data (where your DV is either a 1 or a 0).

<center>

&nbsp;&nbsp;

<iframe width="560" height="315" src="https://youtube.com/embed/L_UmB83NoT0" frameborder="0" data-external="1" allowfullscreen></iframe>

&nbsp;&nbsp;

</center>

Once you've watched the video above, run the code below on your own machines.

### One Factor With Three Levels

In an eye-movement study we measured readersâ€™ regressions as they read sentences. We had sentences conveying three different types of meaning (Negative ve. Neutral vs. Positive). For each, we measures where people did or did not make a regressive eye-movement. This is our DV and is coded as 0 or 1. We had 24 subjects and 24 items.

First, let's read in the data and ensure the appropriate columns are coded as factors.

```{r, message=FALSE}
regressions_data <- read_csv("https://raw.githubusercontent.com/ajstewartlang/16_mixed_models_pt2/master/data/regressions.csv")

tidied_regressions_data <- regressions_data %>%
  mutate(Subject = factor(Subject), Item = factor(Item), 
         Condition = factor(Condition), DV = DV)

str(tidied_regressions_data)
```

Let's work out some descriptives - for each of the three conditions we can calculate the average number of eye-movement regressions (and the associated standard deviation).

```{r}
tidied_regressions_data %>%
  group_by(Condition) %>%
  summarise(mean_DV = mean(DV), sd_DV = sd(DV))
```

So, in terms of the average number of eye-movement regressions, things look pretty similar from condition to condition. Let's build a binomial model to check.

The maximal model (i.e., with random intercepts and slopes for our subjects and for our items) doesn't converge so we need to simplify the random effects structure. The following is the most complex one we can find to fit our data. Note, we are using the `glmer()` function and specifying the distribution family as `binomial`.

```{r}
binomial_model <- glmer(DV ~ Condition + (1 | Subject), 
                        data = tidied_regressions_data,
                        family = binomial)
```

```{r}
summary(binomial_model)
```

So, it doesn't look like there's much going on in our data. We can also compare the binomial model with the fixed effect (above) to a model with only the random effect.

```{r}
null_binomial_model <- glmer(DV ~ (1 | Subject), 
                        data = tidied_regressions_data,
                        family = binomial)
```

We can then use the Likilhood Ratio Test to see if they differ (they don't).

```{r}
anova(binomial_model, null_binomial_model)
```

If our binomial model (or the comparison between our binomial and null model) suggested an effect, we would use the `emmeans()` package to run pairwise comparisons between the different levels of our factor (as we did previously). We would also check our model assumptions by creating a binned residuals plots as done in the video above.

## Ordinal Data
### One Factor With Three Levels

In this final video in this session we will examine how to model ordinal data. Such data are quite common, and people often (incorrectly) model them using ANOVA. However, as the data are ordinal (where differences between adjacent points may not be equivalent), rather than continuous, this is not appropriate. 

<center>

&nbsp;&nbsp;

<iframe width="560" height="315" src="https://youtube.com/embed/kWLLojZrjVw" frameborder="0" data-external="1" allowfullscreen></iframe>

&nbsp;&nbsp;

</center>

Imagine we had 42 participants rate images of sports on a scale of 0-10 corresponding to how much they liked each one. Before each rating measure, they saw a video of a sport - called SportType - that matched or mismatched the one they then had to rate (with a neutral video as baseline). Our DV is `Rating` and our fixed effect is `Condition` (Match vs. Mismatch vs. Neutral).

We're going to use the `{ordinal}` package so let's load that into our library.

```{r, message=FALSE}
library(ordinal)
```

Let's read in our data an ensure our columns are coded appropriately. Our VideoCondition column is currently coded as 2, 3, or 4. 2 corresponds to the Match condition, 3 to Mismatch, and 4 as Neutral. Let's recode the levels of that factor with the labels instead of the numbers.

Note the use of the `select()` function to select the 4 key columns we need for our analysis. We have had to put `dplyr::` beforehand so that R know that it's the `select()` function that's part of the `{dplyr}` package from the `Tidyverse` that we want to use. There's also a function called `select()` in the `{MASS}` package - oftentimes you will have both the `Tidyverse` packages and `{MASS}` loaded so it's important to remember that you may need to make explicit which version of a particular function you want to use. 

```{r, message=FALSE}
ordinal_data <- read_csv("https://raw.githubusercontent.com/ajstewartlang/16_mixed_models_pt2/master/data/ordinal_data.csv")

ordinal_data_tidied <- ordinal_data %>%
  mutate(Subject = factor(Subject), SportsType = factor(SportType)) %>%
  mutate(Ratings = ratings) %>%
  mutate(VideoCondition = as.character(VideoCondition)) %>%
  mutate(VideoCondition = factor(recode(VideoCondition, "2" = "Match", 
                                        "3" = "Mismatch", "4" = "Neutral"))) %>%
  dplyr::select(Subject, SportType, VideoCondition, Ratings)
```

We need to do one more thing before our data set is ready for modelling. We need to make sure that our DV is coded as an ordered factor. We can do that simply with the following. Remember, the name after the `$` symbol refers to the column name in the data frame that preceded the `$`.

```{r}
ordinal_data_tidied$Ratings <- as.ordered(ordinal_data_tidied$Ratings)
```

Let's have a look at our data.

```{r, warning=FALSE}
ordinal_data_tidied %>%
  ggplot(aes(x = VideoCondition, y = Ratings, group = VideoCondition)) +
  geom_jitter(aes(colour = VideoCondition), width = .1, alpha = .25, size = 1) + 
  theme_minimal() +
  guides(colour = FALSE) +
  theme(text = element_text(size = 14)) +
  stat_summary(fun = "median", size = 2, alpha = .5)
```

It does look as if the Mismatch condition has a lower average rating than the other two. Let's build our ordinal mixed model to investigate. Note, we use the `clmm()` function from the `{ordinal}` package to do this. The syntax is pretty similar to that used in the `{lme4}` models.

```{r}
ordinal_model <- clmm(Ratings ~ VideoCondition + 
                        (1 + VideoCondition | Subject) +
                        (1 + VideoCondition | SportType), 
                      data = ordinal_data_tidied)   
```

Let's also build a null model (i.e., with no fixed effects) to see how it compares to the one above.

```{r}
null_ordinal_model <- clmm(Ratings ~ 1 + 
                        (1 + VideoCondition | Subject) +
                        (1 + VideoCondition | SportType), 
                      data = ordinal_data_tidied)   
```

```{r}
anova(null_ordinal_model, ordinal_model)
```

```{r}
summary(ordinal_model)
```

Let's examine which condition differs from each other condition using `{emmeans}`.

```{r}
emmeans(ordinal_model, pairwise ~ VideoCondition)
```

The pairwise comparisons indicate that the Match vs Mismatch conditions differ from each other, as do the Match vs. Neutral conditions. You can change the adjustment for multiple comparisons using the `adjust = ` parameter in `emmeans()`. Does the same story emerge when you use the Bonferroni adjustment?

```{r, eval=FALSE}
emmeans(ordinal_model, pairwise ~ VideoCondition, adjust = "Bonferroni")
```

# Your Challenge
There are two parts to your challenge.  

## Part One

Read in the dataset "factorial_data.csv" from here: 

`https://raw.githubusercontent.com/ajstewartlang/16_mixed_models_pt2/master/data/factorial_data.csv`.  

These data are from a repeated measures experiment where participants had to respond to a target word (measured by our DV which is labelled `Time`).  The target word always followed a prime word. `Prime` and `Target` are our two factors â€“ each with two levels â€“ Positive vs. Negative. We are interested in whether there is a priming effect (i.e., Positive target words responded to more quickly after Positive than after Negative Primes, and Negative target words responded to more quickly after Negative than after Positive Primes). We need to build the appropriate LMM to determine whether this is indeed correct.

Build the appropriate linear mixed model to model the outcome variable (`Time`) and determine whether it was predicted by the fixed effects of `Prime` and `Target`. Critically, we are interested in whether the interaction of these fixed effects predicts our outcome variable.

Remember to set your contrast coding. If you find a singular fit warning, consider simplyfing your random effects structure. When you have a model you are happy with, use the `performance::check_model()` function to check the model assumptions. In your model, you should find a significant interaction effect. Exploring this with `emmeans()` should reveal a priming effect: Positive targets were responded to more quickly following Positive vs. Negative primes, and Negative targets were responded to more quickly following Negative vs. Positive primes. In both cases, this priming effect was around 40ms.

## Part Two
Read in the data file from here:

`https://raw.githubusercontent.com/ajstewartlang/16_mixed_models_pt2/master/data/accuracy_data.csv`

These data are from a study where 32 partcipants saw images of 32 faces and had to classify images as whether each was happy or sad. We want to determine whether people were more accurate (indicatad by a 1 in the Acc column) for Sad or Happy faces (FaceExpression column). Our random effects are `Subject` and `Face` (which corresponds to the trial ID).

Build the appropriate generalised linear mixed model to model the binomial outcome variable (`Acc`) and determine whether it was predicted by the fixed effect of `FaceExpression`. Model both `Subject` and `Face` as random effects. 

Your probably found that you needed to simplify your random effects structure due to over-parameterisation - indicated by a `singular fit` warning. Once you were able to build a model that didn't produce this error, you should have found that accuracy was significantly better for `Sad` than for `Happy` faces.

## Improve this Workshop

If you spot any issues/errors in this workshop, you can raise an issue or create a pull request for [this repo](https://github.com/ajstewartlang/15_mixed_models_pt1). 
